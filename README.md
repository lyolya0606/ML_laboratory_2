# Теоретическая часть

## 1 Эмбеддинги

Embeddings в контексте машинного обучения — это численные векторные представления объектов (слов, изображений, аудиофрагментов и др.), полученные в результате преобразования исходных данных в компактную многомерную форму. Эмбеддинги сохраняют семантические и структурные взаимосвязи между объектами, позволяя эффективно использовать их в качестве входных данных для моделей глубокого обучения. Обучение эмбеддингов осуществляется либо совместно с нейронной сетью, либо предварительно на больших корпусах данных с применением специализированных алгоритмов, таких как Word2Vec, GloVe и Fasttext.

Проще говоря эмбеддинг - это способ преобразования чего-то абстрактного, например слов или изображений в набор чисел и векторов. Эти числа не случайны; они стараются отражают суть или семантику нашего исходного объекта.
В NLP, например, эмбеддинги слов используются для того, чтобы компьютер мог понять, что слова «кошка» и «котенок» связаны между собой ближе, чем, скажем, «кошка» и «окошко». Это достигается путем присвоения словам векторов, которые отражают их значение и контекстное использование в языке.
Эмбеддинги не ограничиваются только словами. В компьютерном зрении, например, можно использовать их для преобразования изображений в вектора, чтобы машина могла понять и различать изображения.

В общем виде, эмбеддинги делятся на:

| Категория                   | Тип            | Описание |
|-----------------------------|----------------|----------|
| **Текстовые эмбеддинги**     | Word Embeddings | Эти эмбеддинги преобразуют слова в векторы, так что слова с похожим значением имеют похожие векторные представления. Они впервые позволили машинам понять семантику человеческих слов. |
| **Текстовые эмбеддинги**     | Sentence Embeddings | Здесь уже идет дело о целых предложениях. Подобные модели создают векторные представления для целых предложений или даже абзацев, улавливая гораздо более тонкие нюансы языка. |
| **Эмбеддинги изображений**   | CNN | CNN позволяет преобразовать изображения в векторы, которые затем используются для различных задач, например, классификации изображений или даже генерации новых изображений. |
| **Эмбеддинги изображений**   | Autoencoders | Автоэнкодеры могут сжимать изображения в более мелкие, плотные векторные представления, которые затем могут быть использованы для различных целей, включая декомпрессию или даже обнаружение аномалий. |
| **Эмбеддинги для других типов данных** | Graph Embeddings | Применяются для работы с графовыми структурами (к примеру, рекомендательные системы). Это способ представить узлы и связи графа в виде векторов. |
| **Эмбеддинги для других типов данных** | Sequence Embeddings | Используются для анализа последовательностей, например, во временных рядах или в музыке. |

---

# Описание разработанной системы

## 1 Bert

Обработка естественного языка одно из востребованных направлений машинного обучения, которое постоянно развивается. В 2018 году компания Google представила новую модель - BERT, сделавшую прорыв в области обработки естественного языка. Несмотря на то, что сейчас у BERT много конкурентов, включая модификации классической модели (RoBERTa, DistilBERT и др.) так и совершенно новые (например, XLNet), BERT всё ещё остается в топе nlp-моделей.
Данная модель применяется во многих задачах: классификация текстов, генерация текста, суммаризация текста и т.д. Но ее также можно использовать для получения векторных представлений текста - эмбеддингов. Они могут использоваться, например, для последующей кластеризации или в качестве исходных данных для других моделей.

BERT представляет собой нейронную сеть, основу которой составляет композиция кодировщиков трансформера. BERT является автокодировщиком. В каждом слое кодировщика применяется двустороннее внимание, что позволяет модели учитывать контекст с обеих сторон от рассматриваемого токена, а значит, точнее определять значения токенов.

### Представление данных:
При подаче текста на вход сети сначала выполняется его токенизация. Токенами служат слова, доступные в словаре, или их составные части — если слово отсутствует в словаре, оно разбивается на части, которые в словаре присутствуют (рисунок 1). Словарь является составляющей модели — так, в BERT-Base используется словарь около 30,000 слов. В самой нейронной сети токены кодируются своими векторными представлениями, а именно, соединяются представления самого токена (предобученные), номера его предложения, а также позиции токена внутри своего предложения. Входные данные поступают на вход и обрабатываются сетью параллельно, а не последовательно, но информация о взаимном расположении слов в исходном предложении сохраняется, будучи включённой в позиционную часть эмбеддинга соответствующего токена.

Выходной слой основной сети имеет следующий вид: поле, отвечающее за ответ в задаче предсказания следующего предложения, а также токены в количестве, равном входному. Обратное преобразование токенов в вероятностное распределение слов осуществляется полносвязным слоем с количеством нейронов, равным числу токенов в исходном словаре.

<div align="center">
  <img src="https://github.com/user-attachments/assets/23dca95b-ba98-46c0-a36f-7814a1ffaa0b">
   <p>Рисунок 1 - Представление входных данных модели</p>
</div>

### Предобучение:

BERT обучается одновременно на двух задачах — предсказания следующего предложения (англ. next sentence prediction) и генерации пропущенного токена (англ. masked language modeling). На вход BERT подаются токенизированные пары предложений, в которых некоторые токены скрыты (рисунок 2). Таким образом, благодаря маскированию токенов, сеть обучается глубокому двунаправленному представлению языка, учится понимать контекст предложения. Задача же предсказания следующего предложения есть задача бинарной классификации — является ли второе предложение продолжением первого. Благодаря ей сеть можно обучить различать наличие связи между предложениями в тексте.

Интерпретация этапа предобучения — обучение модели языку.

### Точная настройка (Fine-tuning)

Этот этап обучения зависит от задачи, и выход сети, полученной на этапе предобучения, может использоваться как вход для решаемой задачи. Так, например, если решаем задачу построения вопросно-ответной системы, можем использовать в качестве ответа последовательность токенов, следующую за разделителем предложений. В общем случае дообучаем модель на данных, специфичных задаче: знание языка уже получено на этапе предобучения, необходима лишь коррекция сети.

Интерпретация этапа fine-tuning — обучение решению конкретной задачи при уже имеющейся общей модели языка.

### Гиперпараметры

Гиперпараметрами модели являются H — размерность скрытого пространства кодировщика, L — количество слоёв-кодировщиков, A — количество голов в механизме внимания.

### Данные и оценка качества

Предобучение ведётся на текстовых данных корпуса BooksCorpus (800 млн. слов), а также на текстах англоязычной Википедии (2.5 млрд. слов). Качество модели авторы оценивают на популярном для обучения моделей обработки естественного языка наборе задач GLUE.

<div align="center">
  <img src="https://github.com/user-attachments/assets/7d38720c-5ff5-4bbb-ace2-12770eb1620c">
   <p>Рисунок 2 - Схема этапа предобучения BERT</p>
</div>

---

# Результаты работы и тестирования системы
В результате выполнения лабораторной работы были получены эмбеддинги Bert и построена нейронная сеть на их основе. Оценка качества модели на тесте Accuracy, Precision, Recall, F1-Score приведена на рисунке 3.

<div align="center">
  <img src="https://github.com/user-attachments/assets/906cdf80-c8fd-495e-bf2b-a30694c77b45">
   <p>Рисунок 3 - Оценка качества модели BERT</p>
</div>

График roc-auc представлен на рисунке 4.

<div align="center">
  <img src="https://github.com/user-attachments/assets/b5c4e965-3531-44cf-9637-83d1e78f85fe">
   <p>Рисунок 4 - График roc-auc модели Bert</p>
</div>

График результата обучения различных слоев представлен на рисунке 5.

<div align="center">
  <img src="https://github.com/user-attachments/assets/7da78bcf-925c-4c1b-9f2b-23866fdf37b1">
   <p>Рисунок 5 - Результат обучения различных слоев</p>
</div>

---

# Выводы

В рамках лабораторной работы были освоены подходы к использованию предобученных языковых моделей и разработке собственных решений на их основе. Применение эмбеддингов BERT дало возможность получать контекстно-зависимые векторные представления текста, что значительно повысило качество обработки естественного языка. Благодаря способности модели точно учитывать порядок и контекст слов, удаётся достичь глубокого понимания смысловых связей в тексте. В результате экспериментов модель продемонстрировала высокую точность классификации (accuracy = 99%) и сбалансированные значения ключевых метрик — precision, recall и F1-score (все на уровне 0.99), что говорит о её высокой обобщающей способности и устойчивости к переобучению.
---

# Использованные источники
- https://ai.mitup.ru/journal/glossary/embeddings/
- https://habr.com/ru/companies/otus/articles/787116/
- https://habr.com/ru/articles/653443/
- http://neerc.ifmo.ru/wiki/index.php?title=BERT_(языковая_модель)
